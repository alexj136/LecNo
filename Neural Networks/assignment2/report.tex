\documentclass{article}

\title{Neural Networks: Assignment 2}
\author{Candidate Number: 18512}

\begin{document}
\maketitle

\begin{centering}
\subsubsection*{Abstract}
\end{centering}
\noindent This report will detail the implementation of a Multi-Layer Perceptron (MLP) and a Radial Basis Function Network (RBFN), in order to predict house prices in the Boston area, a benchmark problem in the field. \\
\indent The implementation is written in Python. No Neural Network libraries or toolkits were used to implement MLP or RBFN, however the Matplotlib library was used to perform principal component analysis (PCA). \\
\indent All error values (unless otherwise stated) are given as mean-squared distance, with unit of thousands of dollars squared. They were produced using 10-fold cross-validation.
\vspace{4mm}

\section*{Pre-Processing}
\noindent Preprocessing was performed with both MLP and RBFN. Experiments were performed using only normalisation, and normalisation with PCA.
\subsection*{Normalisation}
\noindent Normalisation was achieved by first demeaning the data, and then scaling it, such that all values exist between -1 and 1. This allows the network to be trained in far fewer iterations, since very large and very small weight values are not required, which take many iterations to learn. It also increases the performance of the learned function, by preventing features with relatively large values taking precedence over ones with relatively small values, e.g. tax rate (typically around 300) vs number of rooms (typically around 5).
\subsection*{Principal Component Analysis}
\noindent Matplotlib provides an implementation of PCA which was used in this implementation. Where PCA is applied, normalisation has been performed first.

\section*{Multi-Layer Perceptron}
\subsection*{Training Procedure}
\noindent The MLP is trained using sequential backpropagation, as described on page 29 of the lecture slides for MLP. A learning rate of 0.3 was found to be ideal. It is not so large as to prevent the network from converging to an accurate solution (or not converging at all), but is large enough to avoid getting stuck in local minima. \\
\indent Weights were initialised with random values, drawn from a gaussian distribution with $\mu$ = 0 and $\sigma$ = 0.3. \\
\indent A sigmoidal activation function was used because it is differentiable, and gives universal approximation when used with two or more hidden layers. \\
\indent Convergence is typically indicated when the weights change by only a (given) very small amount between each epoch. However, with MLP, since there are many weights, and since the weight values must be checked at each epoch, this is computationally expensive and slows the process down significantly. As a result, I found it more practical not to check the weights for convergence at all, and simply halt iteration after a predetermined number of epochs. This worked fine. I observed that there is a law of diminishing returns here - the effect on error of each additional epoch became negligible after approximately 1000 epochs with the chosen learning rate, and thus I chose to halt iteration after 1000 epochs in my experimentation. \\
\indent To avoid getting stuck in local minima, the order in which the instances were presented for backpropagation was shuffled with each epoch. \\
\subsection*{Performance}
\noindent The MLP structure with the best generalisation was found to have an input layer of size 13 (as this is the dimensionality of the feature data), two hidden layers, the first with 13 neurons, the second with 4 neurons, and an output layer with a single neuron (as the target value is 1-dimensional). More complicated structures tended to have lower training error, but a higher generalisation error - they begin to over-fit the training data and thus generalise poorly. Conversely, simpler structures tended to have higher training \emph{and} generalisation error - they lack the expressive power to capture all nuances and trends in the data that are required for accurate prediction.
\subsection*{Results}
\noindent The table below summarises the results for various network configurations and free parameter choices:
\vspace{4mm} \\
\clearpage
\centerline{
    \begin{tabular}{ | c | c | c | c | c | c | }
        \hline
        MLP Results & & & & & \\
        \hline
        & & Mean Euclidean Distance & & Mean Squared Error & \\
        \hline
        Network Layout & PCA & Training & Generalisation & Training & Generalisation \\
        \hline
        \{13, 1\}         & Yes & 7.55 & 7.00 & 221 & 125 \\
        \{13, 4, 1\}      & Yes & 1.94 & 2.68 & 6.23 & 15.4 \\
        \{13, 13, 1\}     & Yes & 1.44 & 2.78 & 3.45 & 16.8 \\
        \{13, 13, 4, 1\}  & Yes & 1.26 & 2.28 & 2.81 & 12.8 \\
        \{13, 13, 13, 1\} & Yes & 1.26 & 2.41 & 2.73 & 15.6 \\
        \{13, 1\}         & No  & 3.31 & 3.45 & 22.9 & 25.0 \\
        \{13, 4, 1\}      & No  & 2.10 & 2.53 & 8.46 & 14.7 \\
        \{13, 13, 1\}     & No  & 1.88 & 2.51 & 6.54 & 13.6 \\
        \{13, 13, 4, 1\}  & No  & 1.90 & 2.86 & 6.35 & 15.5 \\
        \{13, 13, 13, 1\} & No  & 1.90 & 3.12 & 6.23 & 19.7 \\
        \hline
    \end{tabular}
}
\vspace{4mm}
\noindent The data shows that the best generalisation is given by the network configuration: \{13, 13, 4, 1\}. While it does not have the lowest training error (2.81 vs 2.73 for \{13, 13, 13, 1\} with PCA, 6.35 vs 6.32 without PCA), it does have the lowest generalisation error when used with PCA (12.8). Without PCA, the configuration \{13, 13, 1\} performs has the lowest generalisation error (13.6).

\section*{Radial Basis Function Network}
\subsection*{Training Procedure}
\noindent A gaussian kernel RBFN was used, with the euclidean measure of distance. With more time I would like to have tried using mahalonobis distance. \\
\indent The RBFN was trained by first selecting prototype instances, using the k-means algorithm, choosing instances randomly from the data set as initial prototypes, and halting when the generated clusters contain the same instances for two successive iterations. The generated prototypes were not actual instances from the data set, with the exception of prototypes with only a single instance in their cluster. After the k-means stage, any empty clusters were removed, since they have no effect on the behaviour of the model, but do increase training time as node weights still have to be updated. \\
\indent The weights for the output layer were learned via gradient descent. Here, the learning rate is a much less significant factor than with MLP, since the error function is a second order polynomial there are thus no local minima. A learning rate between 1 and 0.1 was sufficient to learn the weights without an inordinate number of epochs. I used 0.3 in my experiments. \\
\indent Unlike with MLP, it is not too computationally expensive to determine when the weights converge, and thus when the weights did not deviate by more than 0.001 in each case between successive epochs, the algorithm halted, typically with between 10 and 100 epochs, depending on the chosen prototypes and initial weights. \\
\indent As with MLP, weights were initialised with random values, drawn from a gaussian distribution with $\mu$ = 0 and $\sigma$ = 0.3. \\
\subsection*{Performance}
The number of prototypes selected was typically the only significant factor that determined performance. I found that 20 gave the lowest generalisation error. Using less than 20 typically gave greater training and generalisation error, as the learned model does not capture all the trends and nuances in the data. Using more than 20 prototypes greatly reduces training error, and by using every instance as a prototype, a training error of essentially zero can be achieved. However as more prototypes are used, the model begins to learn the noise in the data and generalisation error increases.
\subsection*{Results}
\noindent The table below summarises the results for various RBFN configurations and free parameter choices: \\
\vspace{4mm} \\
\centerline{
    \begin{tabular}{ | c | c | c | c | c | c | }
        \hline
        RBF Results & & & & & \\
        \hline
        Prototypes & PCA & Mean Euclidean Distance & & Mean Squared Error & \\
        \hline
        & & Training & Generalisation & Training & Generalisation \\
        \hline
        1  & Yes & 6.67 & 6.67 & 84.4 & 84.5 \\
        5  & Yes & 5.61 & 5.63 & 63.6 & 64.1 \\
        10 & Yes & 4.79 & 4.88 & 48.9 & 50.3 \\
        20 & Yes & 4.65 & 4.80 & 44.4 & 48.0 \\
        1  & No  & 6.65 & 6.68 & 83.6 & 84.0 \\
        5  & No  & 5.68 & 5.74 & 62.9 & 64.3 \\
        10 & No  & 5.26 & 5.25 & 55.5 & 57.2 \\
        20 & No  & 4.67 & 4.88 & 46.4 & 48.4 \\
        \hline
    \end{tabular}
}
\vspace{4mm} \\
\noindent It can be seen that increasing the number of prototypes reduces both generalisation and training error. Although no data is presented here, using more than 20 prototypes begins to increase generalisation error, while it continues to decrease training error. This is because the model is over-fitting. Interestingly, these show that performing PCA has little to no effect on the ability of RBF to learn the underlying function. The results are little or no better with PCA than without.

\section*{Final Model}
The final model chosen is an MLP with thirteen inputs, two hidden layers, the first with thirteen and the second with four neurons, and an output layer with one neuron, using PCA, because this has the lowest generalisation error of all the configurations discussed. This has a mean squared generalisation error of 12.8, with an average deviation between the prediction value and target value of \$2280.

\section*{Predictions}
Below are the predictions of the final model for the prediction data in the file prediction\_data.htm: \\
\vspace{4mm} \\
\centerline{
    \begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | }
        \hline
        Instance Number  & 1       & 2        & 3       & 4       & 5       & 6       & 7       & 8       \\
        \hline
        CRIM             & 0.03601 & 12.07703 & 0.09743 & 0.39397 & 0.15014 & 0.15042 & 0.37141 & 0.68786 \\
        ZN               & 0.0     & 0.0      & 0.0     & 0.0     & 0.0     & 0.0     & 0.0     & 0.0     \\
        INDUS            & 5.066   & 18.1     & 4.418   & 10.746  & 8.198   & 5.034   & 5.932   & 9.759   \\
        CHAS             & 0.0     & 0.0      & 0.0     & 0.0     & 0.0     & 0.0     & 0.0     & 0.0     \\
        NOX              & 0.4629  & 0.6724   & 0.4524  & 0.5043  & 0.5148  & 0.441   & 0.4925  & 0.5709  \\
        RM               & 6.5558  & 6.0222   & 6.473   & 6.1331  & 6.1047  & 6.6478  & 6.9508  & 6.3932  \\
        AGE              & 45.0    & 89.8     & 49.3    & 60.8    & 60.1    & 40.1    & 67.3    & 69.2    \\
        DIS              & 6.0278  & 2.0613   & 5.1465  & 4.433   & 4.0249  & 6.4959  & 4.4106  & 3.6973  \\
        RAD              & 1.0     & 24.0     & 3.0     & 4.0     & 6.0     & 7.0     & 8.0     & 5.0     \\
        TAX              & 291.0   & 666.0    & 246.0   & 335.0   & 372.0   & 304.0   & 301.0   & 331.0   \\
        PTRATIO          & 17.5    & 20.2     & 18.1    & 19.1    & 17.8    & 18.4    & 17.9    & 16.5    \\
        B                & 389.27  & 288.08   & 392.41  & 382.72  & 387.36  & 388.43  & 385.27  & 369.19  \\
        LSTAT            & 7.37    & 18.6     & 9.07    & 12.19   & 12.3    & 7.98    & 7.96    & 10.65   \\
        \hline
        MEDV (predicted) & 25.4    & 14.0     & 25.5    & 21.4    & 21.6    & 25.9    & 30.0    & 23.2    \\
        \hline
    \end{tabular}
}


\section*{Appendix}
\subsection*{File: main.py}
\begin{verbatim}

from instances  import *
from mlp        import *
from rbf        import *
from preprocess import *
from misc       import *

def testNetEuclidean(net, testInsts):
    '''Test a network with the given test instances. Return the mean
    euclidean distance of the target output from the actual output.'''
    return sum([euclideanDist(net.fwdPass(inst), inst.label)
            for inst in testInsts])/len(testInsts)

def testNetMeanSquared(net, testInsts, ppr):
    '''Test a network with the given test instances. Return the mean
    squared error. Requires a Preprocessor object to rescale values.'''

    if isinstance(ppr, Preprocessor):
        factor = ppr.scaleInst.label[0]
    else:
        factor = ppr.preproc.scaleInst.label[0]

    return sum([sum([math.pow((t * factor) - (l * factor), 2)
            for t, l in zip(net.fwdPass(inst), inst.label)])
            for inst in testInsts])/len(testInsts)

def crossVal(net, numBins, insts, rate, convergenceThreshold, maxIters,
        usePCA=False):
    '''Perform cross-validation of the given network, with the given number
    of bins, with the given instances, with the given learning rate, with the
    given convergence threshold, and the given maximum number of iterations.
    Works for RBFNetworks and MLPNetworks.'''

    assert numBins > 1 # Can't cross-validate with less than two bins

    sets = bins(numBins, insts)
    eucTrainErrors = [] # Euclidean distance training errors
    eucGenErrors   = [] # Euclidean distance generalisation errors
    msqTrainErrors = [] # Mean squared training errors
    msqGenErrors   = [] # Mean squared generalisation errors

    for setIndex in range(numBins):
        testInsts  = sets[setIndex]
        trainInsts = flatten(sets[:setIndex] + sets[setIndex + 1:])

        if usePCA:
            pprTestInsts, ppr = pcaPreprocess(trainInsts)
            pprTrainInsts = pcaPprWith(testInsts, ppr)
        else:
            pprTestInsts, ppr = preprocess(trainInsts)
            pprTrainInsts = pprWith(testInsts, ppr)

        net.reinitialise()
        net.train(pprTrainInsts, rate, convergenceThreshold, maxIters)

        eucTrainErrors.append(testNetEuclidean(net, pprTrainInsts))
        eucGenErrors.append(testNetEuclidean(net, pprTestInsts))
        msqTrainErrors.append(testNetMeanSquared(net, pprTrainInsts, ppr))
        msqGenErrors.append(testNetMeanSquared(net, pprTestInsts, ppr))

    eucTrainErr = sum(eucTrainErrors)/len(eucTrainErrors)
    eucGenErr   = sum(eucGenErrors)/len(eucGenErrors)
    msqTrainErr = sum(msqTrainErrors)/len(msqTrainErrors)
    msqGenErr   = sum(msqGenErrors)/len(msqGenErrors)

    if usePCA:
        return eucTrainErr * ppr.preproc.scaleInst.label[0], \
                eucGenErr * ppr.preproc.scaleInst.label[0], \
                msqTrainErr, msqGenErr
    else:
        return eucTrainErr * ppr.scaleInst.label[0], \
                eucGenErr * ppr.scaleInst.label[0], \
                msqTrainErr, msqGenErr

if __name__ == '__main__':

    # Perform tests of this implementation  - various MLP layouts & RBF
    # configurations. Should run in about 2 hours.

    insts = parseTrainingData()
    predInsts = parsePredictionData()

    # Test of chosen model on prediction data

    mlp = MLPNetwork(0, 0.3, [13, 13, 4, 1])
    pcaTrainInsts, ppr = pcaPreprocess(insts)
    pcaTestInsts = pcaPprWith(predInsts, ppr)
    mlp.train(pcaTrainInsts, 0.3, None, 1000)
    for inst, pcaInst in zip(predInsts, pcaTestInsts):
        print 'INST:', inst.data
        print 'PRED:', (mlp.fwdPass(pcaInst)[0] * \
                ppr.preproc.scaleInst.label[0]) + ppr.preproc.meanInst.label[0]

    # Cross-validation tests of various configurations

    for protos, withPCA in [(1, True), (5, True), (10, True), (20, True), \
            (1, False), (5, False), (10, False), (20, False)]:

        rbf = RBFNetwork(0, 0.3, protos, 1)
        eucTrainErr, eucGenErr, msqTrainErr, msqGenErr = \
                crossVal(rbf, 10, insts, 0.3, 0.001, 100, usePCA=withPCA)
        print 'RBF: PROTOS:', str(protos), 'PCA:', str(withPCA)
        print 'Euclidean Training error:', eucTrainErr
        print 'Euclidean Generalisation error:', eucGenErr
        print 'Mean Squared Training error:', msqTrainErr
        print 'Mean Squared Generalisation error:', msqGenErr

    for lyt, withPCA in [([13,         1], True ), ([13, 4,      1], True ), \
                         ([13, 13,     1], True ), ([13, 13, 4,  1], True ), \
                         ([13, 13, 13, 1], True ), ([13,         1], False), \
                         ([13, 4,      1], False), ([13, 13,     1], False), \
                         ([13, 13, 4,  1], False), ([13, 13, 13, 1], False)]:

        mlp = MLPNetwork(0, 0.3, lyt)
        eucTrainErr, eucGenErr, msqTrainErr, msqGenErr = \
                crossVal(mlp, 10, insts, 0.3, None, 1000, usePCA=withPCA)
        print 'MLP: LAYOUT:', str(lyt), 'PCA:', str(withPCA)
        print 'Euclidean Training error:', eucTrainErr
        print 'Euclidean Generalisation error:', eucGenErr
        print 'Mean Squared Training error:', msqTrainErr
        print 'Mean Squared Generalisation error:', msqGenErr

\end{verbatim}

\subsection*{File: rbf.py}
\begin{verbatim}

import math
from copy   import deepcopy
from kmeans import kMeans
from mlp    import Node
from misc   import euclideanDist, meanInst, flatten

class RBFNetwork:
    def __init__(self, mean, stdDev, numProtos, numOutputs):
        '''Create a new RBFNetwork'''
        self.mean = mean
        self.stdDev = stdDev
        self.numProtos = numProtos
        self.numOutputs = numOutputs
        self.reinitialise()

    def reinitialise(self):
        '''Reset the parameters - 'un-train' - this network'''
        self.rbfNodes = None
        self.wtSumNodes = [Node.gaussWtsNode(self.mean, self.stdDev, \
                self.numProtos) for x in range(self.numOutputs)]

    @property
    def isTrained(self):
        '''Determine whether or not this RBFNN has been trained.'''
        return self.rbfNodes is None or self.wtSumNodes is None

    def passRBFLayer(self, inst):
        '''Pass an instance through the RBF layer of this RBFNetwork, to obtain
        response values for each node.'''
        return [node.activation(inst) for node in self.rbfNodes]

    def fwdPass(self, inst):
        '''Given an input instance, calculate all node activations to produce an
        output vector - a prediction for the given instance'''
        rbfNodeOutputs = self.passRBFLayer(inst)
        return [node.activation(rbfNodeOutputs) for node in self.wtSumNodes]

    def train(self, insts, rate, convergenceThreshold, maxIters):
        '''Train this RBFNN - calculate beta values for each RBF node, and
        perform gradient descent to learn weights for the weighted sum nodes.
        The wtMean and wtStdDev parameters are the mean and standard deviation
        of the gaussian distribution from which initial weights for the weighted
        sum nodes will be randomly drawn.'''

        protos, clusters = kMeans(self.numProtos, insts)

        # Filter empty clusters
        newProtos = []
        newClusters = []
        toRemove = [False if len(c) == 0 else True for c in clusters]
        for idx, shouldKeep in enumerate(toRemove):
            if shouldKeep:
                newProtos.append(protos[idx])
                newClusters.append(clusters[idx])
        protos = newProtos
        clusters = newClusters

        # Calculate beta coefficients
        betas = []
        for cluster in clusters:
            # If the cluster is empty, make the beta coefficient equal 1, which
            # will cause the activation of this node decrease very sharply as
            # the given instance gets further from the prototype, effectively
            # rendering that prototype irrelevant.
            if len(cluster) == 0:
                betas.append(0)
            else:
                clusterMean = meanInst(cluster)
                dists = [euclideanDist(inst.data, clusterMean.data)
                        for inst in cluster]
                sigma = sum(dists)/len(cluster)
                if sum(dists) == 0:
                    betas.append(1)
                else:
                    betas.append(1.0 / (2 * math.pow(sigma, 2)))

        # Create the RBF nodes from the prototype & beta coefficient
        self.rbfNodes = [RBFNode(proto, beta)
                for proto, beta in zip(protos, betas)]


        # Perform gradient descent to learn weights for the output nodes.
        conv = ConvergenceTester(convergenceThreshold)
        for x in range(maxIters):

            rbfOutputs = [[1] + self.passRBFLayer(inst) for inst in insts]
            predictions = [self.fwdPass(inst) for inst in insts]

            for outputIndex, node in enumerate(self.wtSumNodes):

                for wtIdx in range(len(node.wts)):
                    node.wts[wtIdx] -= (rate * (sum([( \
                            predictions[i][outputIndex] - \
                            inst.label[outputIndex]) * rbfOutputs[i][wtIdx] \
                            for i, inst in enumerate(insts)])/len(insts)))

            if conv.test(flatten([node.wts for node in self.wtSumNodes])): break

class RBFNode:
    def __init__(self, proto, beta):
        '''Create a new RBFNode'''
        self.proto = proto
        self.beta = beta

    def activation(self, inst):
        '''The phi function - a gaussian activation function is used here, with
        its mean at the prototype for this instance.'''
        return math.pow(math.e, -1 * self.beta * \
                math.pow(euclideanDist(inst.data, self.proto.data), 2))

class ConvergenceTester:
    '''Class used to determine if an algorithm has converged to a set of
    weights. Stores values used to test against the next value, and a threshold
    of allowed deviation between the stored and the presented value within which
    convergence is indicated.'''
    def __init__(self, threshold):
        self.threshold = threshold
        self.storedValues = None

    def test(self, values):
        '''Test to see if convergence has occured'''

        if self.storedValues is None:
            self.storedValues = deepcopy(values)
            return False

        else:
            assert len(self.storedValues) == len(values)
            result = max([abs(a - b) for a, b in \
                    zip(self.storedValues, values)]) < self.threshold
            self.storedValues = deepcopy(values)
            return result

\end{verbatim}

\subsection*{File: mlp.py}
\begin{verbatim}

from instances import *
from random import gauss as gaussRandom, shuffle
import numpy as np
import math
from misc import euclideanDist

class MLPNetwork:
    def __init__(self, mean, stdDev, layout):
        '''Create a network with specified layout, where every weight (including
        biases) is initialised with a random number drawn from a normal
        distribution with the specified mean and standard deviation'''
        self.mean = mean
        self.stdDev = stdDev
        self.layout = layout
        self.reinitialise()

    def reinitialise(self):
        '''Reset the parameters - 'un-train' - this network'''
        self.layers = [Layer.gaussWtsLayer(self.mean, self.stdDev, \
                self.layout[x-1], self.layout[x]) \
                for x in range(1, len(self.layout))]

    def __str__(self):
        '''Get a nice string representation of this Network object'''
        return ''.join([''.join(["--- LAYER ", str(x), " ---\n",
            str(self.layers[x]), "\n"]) for x in range(len(self.layers))])

    def printDeltas(self):
        '''Print all nodes with their current delta values'''
        layerNo = 0
        for layer in self.layers:
            print '--- LAYER', str(layerNo), '---'
            layerNo = layerNo + 1

            nodeNo = 0
            for node in layer.nodes:
                print 'NODE', str(nodeNo), '- DELTA =', str(node.delta)
                nodeNo = nodeNo + 1

    def layer(self, layerNo):
        '''Get the layer of the given number'''
        return self.layers[layerNo]

    @property
    def outputLayer(self):
        '''Get the output layer of this Network'''
        return self.layers[len(self.layers) - 1]

    def fwdPass(self, inst):
        '''Given an input instance, calculate all node activations to produce an
        output vector - a prediction for the given instance'''

        vec = inst.data

        for layer in self.layers:

            for node in layer.nodes:
                node.activn = node.activation(vec)

            # Don't apply the sigmoid function to the output of the last layer.
            if layer is self.outputLayer:
                vec = [node.activn for node in layer.nodes]
            else:
                vec = [sigmoid(node.activn) for node in layer.nodes]

        return vec

    def train(self, insts, rate, convergenceThreshold, maxIters):
        '''Train the network using the back propagation algorithm, for a given
        set of instances and a given learning rate. Stop when the convergence
        threshold is reached, or when the maximum allowed number of iterations
        (epochs) have been reached.'''

        if convergenceThreshold is not None:
            print 'WARNING: Convergence threshold not implemented for MLP'

        for x in range(maxIters):
            
            for inst in insts:
                
                # Randomising the order in which the instances are presented 
                # reduces the risk of getting stuck in a local minimum
                shuffle(insts)

                # Pass the instance through the network so that node activations
                # correspond to this instance, and to get the network's output
                # for this instance
                out = self.fwdPass(inst)

                # Recalculate delta values for the output layer
                for nodeNo, node in enumerate(self.outputLayer.nodes):
                    node.delta = -1 * derivSigmoid(node.activn) * (
                            inst.label[nodeNo] - out[nodeNo])

                # Recalculate delta values for the hidden layers
                for layerNo in range(len(self.layers) - 2, -1, -1):
                    layer = self.layers[layerNo]

                    for nodeNo, node in enumerate(layer.nodes):
                        node.delta = derivSigmoid(node.activn) * sum([
                            (toNode.delta * toNode.wts[nodeNo + 1])
                            for toNode in self.layers[layerNo + 1].nodes])

                # Update the weights for the input layer
                for node in self.layers[0].nodes:
                    node.wts[0] = node.wts[0] - (rate * node.delta)
                    for inputVal in range(len(inst.data)):
                        node.wts[inputVal + 1] -= rate * node.delta * \
                                inst.data[inputVal]

                # Update the weights for the layers after the input layer
                for layerNo in range(1, len(self.layers)):
                    for nodeNo, node in enumerate(self.layers[layerNo].nodes):

                        node.wts[0] -= rate * node.delta
                        wtIndex = 1
                        for inputNode in self.layers[layerNo - 1].nodes:
                            node.wts[wtIndex] -= rate * node.delta * \
                                    sigmoid(inputNode.activn)
                            wtIndex += 1

class Layer:
    def __init__(self, nodes):
        '''Build a new Layer from a list of nodes'''
        self.nodes = nodes

    def __str__(self):
        '''Get a nice string representation of this Layer object'''
        return ''.join([''.join([str(nd), "\n"]) for nd in self.nodes])

    def node(self, nodeNo):
        '''Get the node of the given number'''
        return self.nodes[nodeNo]

    @staticmethod
    def gaussWtsLayer(mean, stdDev, numWtsPerNode, numNodes):
        '''Create a layer with the specified number of nodes, each with the
        specified number of weights, where the weights for each node are
        initialised with values that are each a normally distributed random
        number drawn from a distribution with the specified mean and standard
        deviation'''
        return Layer([Node.gaussWtsNode(mean, stdDev, numWtsPerNode)
            for x in range(numNodes)])

class Node:
    '''The Node class represents a single node within a neural network. A node
    object has a single member - a list of weights. The convention is that the
    first element of that list is the bias of this Node.'''

    def __init__(self, wts):
        '''Build a new Node from a weight vector'''
        self.wts    = wts
        self.delta  = None  # It is very helpful when doing backpropagation, to
        self.activn = None  # be able to store delta and activation values in
                            # the nodes that they correspond to

    def __str__(self):
        '''Get a nice string representation of this Node object'''
        return ''.join(['B: ', str(self.wts[0]), ', WTS: ', str(self.wts[1:])])

    def wt(self, wtNo):
        '''Get the weight of the given number (as usual, 0 is the index of the
        bias weight)'''
        return self.wts[wtNo]

    def setWt(self, wtNo, val):
        '''Set the weight of the given number (as usual, 0 is the index of the
        bias weight) to the given value'''
        self.wts[wtNo] = val

    @staticmethod
    def gaussWtsNode(mean, stdDev, numInputs):
        '''Create a single network node with a specified number of inputs. The
        Node will have one more weight than numInputs - the extra weight is the
        bias for this Node. Weights are initialised with normally distributed
        random values, drawn from a distribution of given mean and standard
        deviation'''
        # + 1 for bias weight
        return Node([gaussRandom(mean, stdDev) for x in range(numInputs + 1)])

    def activation(self, vec):
        '''Compute the activation of the given input vector for this node. The
        given input vector may be an instance or a vector of values from a
        previous layer. The only constraint is that the dimensionality of the
        input must match the number of weights (not including the bias) of this
        node. The retured value is the dot product of the input vector with the
        weight vector, plus the bias, fed into a sigmoid function.'''
        return np.dot(vec, self.wts[1:]) + self.wts[0]

def sigmoid(x, k=1):
    '''The sigmoid function is defined as:
        sigmoid(x) = 1 / 1 - e^(-1 * k * x)
    where 'x' is a variable and 'k' and is an optionally specified coefficient.
    The function is used when computing the activation of a Node.'''
    if   x < -100: return 0
    elif x >  100: return 1
    else         : return 1 / (1 + math.pow(math.e, -1 * k * x))

def derivSigmoid(x, k=1):
    '''Compute the value of the derivative of the sigmoid function at a given x,
    with optionally specified constant k. The derivative of the sigmoid function
    can be shown to be:
        sig'(x) = k * sig(x) * (1 - sig(x))
    where k is the same coefficient used in the sigmoid function.'''
    sigX = sigmoid(x, k)
    return k * sigX * (1 - sigX)

\end{verbatim}

\subsection*{File: kmeans.py}
\begin{verbatim}

from random    import uniform, randint
from copy      import deepcopy
from instances import *
from misc      import *

def minmax(insts):
    '''Return an instance with feature values that are the minimum of all
    feature and label values across the given instances. Return an instance with
    maximum values also.'''

    minFeats = []
    maxFeats = []

    for feat in range(len(insts[0].data)):
        feats = [inst.data[feat] for inst in insts]
        minFeats.append(min(feats))
        maxFeats.append(max(feats))

    return Instance(minFeats, None), Instance(maxFeats, None)

def kRandomPrototypes(k, insts):
    '''Choose k instances from the given list at random to be prototypes.
    The returned Instances are deep copies of the chosen Instances. They do not
    have label values.'''
    return [Instance(deepcopy(insts[i].data), None) for i in \
            [randint(0, len(insts) - 1) for x in range(k)]]

def kMeans(k, insts):
    '''Perform k-means clustering on the given instances. Return the k prototype
    instances (these are not instances from the data set itself).'''

    protos = kRandomPrototypes(k, insts)

    prevClustering = bins(k, insts)
    converged = False

    while not converged:

        # Assignment step - put each instance in the cluster corresponding to
        # its closest prototype
        newClustering = [[] for x in range(k)]
        for inst in insts:
            # Find the prototype closest to this instance and add the instance
            # to the corresponding cluster
            bestIdx = 0
            bestDist = euclideanDist(inst.data, protos[bestIdx].data)
            for idx in range(len(protos)):

                # None prototypes are produced if clusters are empty, so set
                # curDist = bestDist + 1 so we don't put the instance in this
                # cluster
                if protos[idx] is None:
                    curDist = bestDist + 1
                else:
                    curDist = euclideanDist(inst.data, protos[idx].data)

                if curDist < bestDist:
                    bestIdx  = idx
                    bestDist = curDist

            newClustering[bestIdx].append(inst)

        # Recompute the prototypes to be mean values of data in their cluster.
        # If their cluster is now empty, do not update the prototype.
        for idx in range(len(newClustering)):
            meanI = meanInst(newClustering[idx])
            if meanI is not None:
                protos[idx] = meanI

        # If the current clusters contain the same elements as they did last
        # time, we've converged
        foundDiff = False
        prevClusterSets = map(set, prevClustering)
        for cluster in newClustering:
            if set(cluster) not in prevClusterSets:
                foundDiff = True
                break
        converged = not foundDiff

        prevClustering = newClustering

    return protos, newClustering

if __name__ == '__main__':
    insts = parseTrainingData()
    protos = kMeans(2, insts)
    for proto in protos: print proto

\end{verbatim}

\subsection*{File: preprocess.py}
\begin{verbatim}

import numpy as np
from matplotlib.mlab import PCA
from instances       import Instance
from copy            import deepcopy
from misc            import meanInst

class Preprocessor:
    '''A preprocessor object is generated by the preprocess() function, so that
    subsequent preprocessing can be performed by calling preprocessWith(), with
    the generated Preprocessor object as the second parameter. This will ensure
    that the mappings applied to the instances preprocessed with
    preprocessWith() are identical to those used to preprocess the instances
    initially preprocessed with preprocess().'''
    def __init__(self, meanInst, scaleInst):
        self.meanInst  = meanInst
        self.scaleInst = scaleInst

def preprocess(insts):
    '''Preprocess a list of Instances, returning a Preprocessor object that can
    be used to preprocess more instances with the same mapping.'''
    demeanedInsts, meanInst = demean(insts)
    scaledInsts, scaleInst = scale(demeanedInsts)
    return scaledInsts, Preprocessor(meanInst, scaleInst)

def pprWith(insts, preproc):
    '''Preprocess a list of instances according to the mapping described by the
    given Preprocessor object.'''
    demeanedInsts = [demeanNewInst(inst, preproc.meanInst) for inst in insts]
    return [scaleNewInst(inst, preproc.scaleInst) for inst in demeanedInsts]

def unppr(inst, preproc):
    '''Re-scale and re-mean an instance, returning an instance in the original
    domain.'''
    cpy = deepcopy(inst)
    for i in range(len(cpy.data)):
        cpy.data[i] = (cpy.data[i] * preproc.scaleInst.data[i]) + \
                preproc.meanInst.data[i]
    for i in range(len(cpy.label)): 
        cpy.label[i] = cpy.label[i] * preproc.scaleInst.label[i] + \
                preproc.meanInst.label[i]
    return cpy

def demean(insts):
    '''Produce a new list of instances from the given ones, that have been
    demeaned. Return a new list, and an Instance object that represents the mean
    values of the data from the original instances, that can be used to demean
    unseen instances, or to recover values in the original domain'''

    meanI = meanInst(insts)

    # Create new instances from the old ones and adjust all values appropriately
    newInsts = deepcopy(insts)
    for inst in newInsts:
        for d in range(len(inst.data)):
            inst.data[d] -= meanI.data[d]
        for l in range(len(inst.label)):
            inst.label[l] -= meanI.label[l]
    
    return newInsts, meanI

def demeanNewInst(inst, means):
    '''Given an unseen Instance and an example mean Instance, return a new
    instance in the same domain as the data that produced the example
    Instance'''
    newInst = deepcopy(inst)
    for feat in range(len(newInst.data)):
        newInst.data[feat] -= means.data[feat]
    for lbl in range(len(newInst.label)):
        newInst.label[lbl] -= means.label[lbl]
    return newInst

def scale(insts):
    '''Produce a list of Instances that are scaled versions of the given
    Instances. Return the new instances and an example Instance that stores the
    scale factors for each dimension of the given instances.'''

    # We record the scale factor for feature and return them in a list, so
    # that we can (a) apply an identical transformation to unseen instances and
    # (b) recover the feature values as they were before scaling
    featScales = []

    for feat in range(len(insts[0].data)):

        # Find the maximum value for this feature, which we will use as our
        # scale factor
        absMax = 0
        for inst in insts:
            if abs(inst.data[feat]) > absMax:
                absMax = abs(inst.data[feat])

        # Record this scale factor
        featScales.append(absMax)

    labelScales = []    

    for lbl in range(len(insts[0].label)):

        # Find the maximum value for this target dimension, which we will use as
        # our scale factor
        absMax = 0
        for inst in insts:
            if abs(inst.label[lbl]) > absMax:
                absMax = abs(inst.label[lbl])

        # Record this scale factor
        labelScales.append(absMax)

    # Create new instances from the old ones and adjust all values appropriately
    newInsts = deepcopy(insts)
    for inst in newInsts:
        for d in range(len(inst.data)):
            inst.data[d] /= featScales[d]
        for l in range(len(inst.label)):
            inst.label[l] /= labelScales[l]

    return newInsts, Instance(featScales, labelScales)

def scaleNewInst(inst, scaleFacs):
    '''Given an instance and a scale factor example Instance, for data returned
    by scale(), return a new instance that is equivalent to the given instance,
    but in the scaled domain.'''
    newInst = deepcopy(inst)
    for feat in range(len(newInst.data)):
        newInst.data[feat] /= scaleFacs.data[feat]
    for lbl in range(len(newInst.label)):
        newInst.label[lbl] /= scaleFacs.label[lbl]
    return newInst

class PCAPreprocessor:
    '''The PCAPreprocessor class functions in much the same way as the
    Preprocessor class, except that it performs PCA as well as normalisation.'''

    def __init__(self, pcaObj, preproc):
        '''Create a new PCAPreprocessor object, which stores the object returned
        by a call to matplotlib.mlab.PCA, and the Preprocessor object used to
        normalise the label of the preprocessed instances.'''
        self.pcaObj = pcaObj
        self.preproc = preproc

def pcaPreprocess(insts):
    '''Preprocess some instances using PCA and normalisation. Return the
    transformed data and an object that can be used to perform the same
    transformation on other unseen instances.'''

    # Normalise the instances
    normInsts, preproc = preprocess(insts)

    # Perform PCA on the feature information of the given instances
    pcaObj = PCA(np.array(deepcopy([inst.data for inst in normInsts])))

    # Replace the old feature data with the PCA'd feature data
    pcaInsts = deepcopy([Instance(pcaData, inst.label)
            for pcaData, inst in zip(pcaObj.Y.tolist(), normInsts)])

    return pcaInsts, PCAPreprocessor(pcaObj, preproc)

def pcaPprWith(insts, pcaPreproc):

    # Normalise
    normInsts = pprWith(insts, pcaPreproc.preproc)

    # Project the new feature data into the PCA axis
    pcaData = map(pcaPreproc.pcaObj.project, [i.data for i in normInsts])

    # Replace the old feature data with the PCA'd feature data
    pcaInsts = deepcopy([Instance(pcaD, inst.label)
            for pcaD, inst in zip(pcaData, normInsts)])

    # Return normalised PCA'd data
    return pcaInsts

\end{verbatim}

\subsection*{File: instances.py}
\begin{verbatim}

class Instance:
    def __init__(self, data, label):
        '''Build an instance from a list of features and a list of target
        values'''
        self.data  = data   # The feature values - a list of numbers
        self.label = label  # The label/target value(s) - also a list of numbers

    def __str__(self):
        '''Get a nice string representation of this Instance object'''
        return ''.join(['DATA: ', str(self.data), ', LABEL: ', str(self.label)])
    
def parseTrainingData():
    '''Parse the file training_instances.txt into a list of Instance objects'''
    # Get the file as a string
    with open('training_data.txt', 'r') as f: text = f.read()

    # Split the string into a list of lines
    lines = text.split('\n')

    # Split each line into a list of strings, removing any empty lines
    splitLines = filter(lambda ln: ln != [], map(lambda s: s.split(), lines))

    # Parse a float from each string
    floatMatrix = map(lambda ln: map(float, ln), splitLines)

    # Ensure that all the instances have the same dimensionality
    ensureSameDimensionality(floatMatrix)

    # Break the float matrix into separate feature and target value matrices
    featureMatrix = []
    targetValMatrix = []
    for instData in floatMatrix:
        featureMatrix.append(instData[:len(instData) - 1])
        targetValMatrix.append([instData[len(instData) - 1]])

    # Convert the data matrix into a list of instances and return it
    return map(Instance, featureMatrix, targetValMatrix)

def parsePredictionData():
    '''Parse the file training_instances.txt into a list of Instance objects'''
    # Get the file as a string
    with open('prediction_data.txt', 'r') as f: text = f.read()

    # Split the string into a list of lines
    lines = text.split('\n')

    # Split each line into a list of strings, removing any empty lines
    splitLines = filter(lambda ln: ln != [], map(lambda s: s.split(), lines))

    # Parse a float from each string
    floatMatrix = map(lambda ln: map(float, ln), splitLines)

    # Ensure that all the instances have the same dimensionality
    ensureSameDimensionality(floatMatrix)

    # Convert the data matrix into a list of instances and return it
    return [Instance(d, []) for d in floatMatrix]

def ensureSameDimensionality(valueMatrix):
    '''Guarantee that, for a list of lists, all lists have the same length'''
    dimensionality = len(valueMatrix[0])
    for row in valueMatrix:
        if len(row) != dimensionality:
            raise Exception('Parsed data has varying dimensionality')

\end{verbatim}

\subsection*{File: misc.py}
\begin{verbatim}

from instances import Instance
import math

xorData = \
        [ Instance([-1, -1], [-1])
        , Instance([-1,  1], [ 1])
        , Instance([ 1, -1], [ 1])
        , Instance([ 1,  1], [-1])
        ]

def euclideanDist(x, y):
    '''Compute the euclidean distance between two vectors (lists) x and y'''
    return math.sqrt(sum(map(lambda xi, yi: math.pow(xi - yi, 2), x, y)))

def bins(n, data):
    '''Break a single list up into a list of n lists, differing in size by no
    more than 1. Does not modify the original list, but data is not copied.
    Ordering of elements in output is arbitrary.'''
    sets = [[] for x in range(n)]

    setIndex = 0
    elemIndex = 0

    while elemIndex < len(data):
        sets[setIndex].append(data[elemIndex])
        elemIndex = elemIndex + 1
        setIndex = setIndex + 1 if setIndex + 1 < len(sets) else 0

    return sets

def flatten(lists):
    '''Flatten a list of lists into one list, maintaining ordering. As with
    bins(), the given list is not modified, but data is not copied.'''
    return [elem for lst in lists for elem in lst]

def meanInst(insts):
    '''Create a prototype/mean Instance for the given Instances'''
    if len(insts) == 0: return None

    featMeans = []

    for feat in range(len(insts[0].data)):

        total = 0
        for inst in insts:
            total = total + inst.data[feat]

        mean = total / len(insts)
        featMeans.append(mean)

    labelMeans = []    

    for lbl in range(len(insts[0].label)):

        total = 0
        for inst in insts:
            total = total + inst.label[lbl]

        mean = total / len(insts)
        labelMeans.append(mean)
    
    return Instance(featMeans, labelMeans)

\end{verbatim}

\end{document}
