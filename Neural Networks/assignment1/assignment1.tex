\documentclass{article}

\title{Neural Networks: Assignment 1}
\author{Candidate Number: 18512}

\begin{document}
\maketitle

\section*{Introduction}
This report will detail the implementation of a single layer perceptron for the purposes of binary classification and linear regression. \\
\indent The perceptron was implemented in Python. No Neural Network libraries or toolkits were used. Matplotlib and Numpy were used to produce graphs.

\section*{Part A: Classification}
\subsection*{Question 1}
Both sequential \& batch gradient descent algorithms were used in this implementation for the sake of comparison between the two. \\
In both cases, the Perceptron Criterion error function is used, because unlike more naive error functions, Perceptron Criterion function yields a value proportional to the (sum of the) distance from the decision boundary. This is useful because any change in weights will correspond to a change in the value that the error function yields, which allows us to calculate the gradient of the error function, which in turn allows us to learn the optimum decision boundary via gradient descent. If a change in weights does not correspond to a change in error, we cannot determine whether the weight adjustment improved or worsened our learned function. \\
Initial weights were all zero as specified in the assignment brief. \\
The choice of learning rate for this task is not terribly important - it should be large enough that it does not cause the algorithm to do unnecessary extra iterations, and not be so great as to prevent convergence. 0.1 was found to be sufficient.

\subsubsection*{Training Procedure}
The training procedure for sequential gradient descent is as follows:

\begin{verbatim}
Procedure GradientDescentSequential(trainingData, weights, learningRate):
    while procedure not converged:
        for each incorrectly classified instance in trainingData:
            update weights in proportion to learningRate so
            instance is (closer to being) correctly classified
    output weights
\end{verbatim}

\subsubsection*{Learnable Patterns}
The implemented single layer perceptron was able to learn four of the six given patterns:
\begin{center}
    \begin{tabular}{ l | c c c | l | c c c }
                & X1 & X2 & Label &         & X1 & X2 & Label \\
        \hline
        Pattern & 1  & 1  & +     & Pattern & 1  & 1  & -     \\
        Set 1   & 1  & 0  & -     & Set 2   & 1  & 0  & +     \\
                & 0  & 1  & +     &         & 0  & 1  & -     \\
                & 0  & 0  & -     &         & 0  & 0  & +     \\
        \hline
        Pattern & 1  & 1  & +     & Pattern & 1  & 1  & -     \\
        Set 3   & 1  & 0  & +     & Set 4   & 1  & 0  & -     \\
                & 0  & 1  & -     &         & 0  & 1  & +     \\
                & 0  & 0  & -     &         & 0  & 0  & +     \\
    \end{tabular}
\end{center}
Pattern sets 1 \& 2 are equivalent modulo the choice of class names, and as a result are learnt in the same number of iterations (3). The same is the case for pattern sets 3 \& 4. The ones that cannot be learnt are just so because they are not linearly separable i.e. there exists no linear boundary that separates all instances of one class from all instances of the other. These are given below:
\begin{center}
    \begin{tabular}{ l | c c c | l | c c c }
                & X1 & X2 & Label &         & X1 & X2 & Label \\
        \hline
        Pattern & 1  & 1  & +     & Pattern & 1  & 1  & -     \\
        Set 5   & 1  & 0  & -     & Set 6   & 1  & 0  & +     \\
                & 0  & 1  & -     &         & 0  & 1  & +     \\
                & 0  & 0  & +     &         & 0  & 0  & -     \\
    \end{tabular}
\end{center}
The following table shows the learned weights and number of iterations for each set of learnable patterns:
\begin{center}
    \begin{tabular}{ c | c | c | c | c }
        Pattern Set & Bias & Weight 1 & Weight 2 & Iterations \\
        \hline
        1           & 0    & -0.2     & 0        & 3          \\
        \hline
        2           & -0.1 & 0.2      & 0        & 3          \\
        \hline
        3           & 0    & 0        & -0.1     & 3          \\
        \hline
        4           & -0.1 & 0        & 0.1      & 3          \\
    \end{tabular}
\end{center}

\subsection*{Question 2}

\section*{Part B: Regression}
\subsection*{Question 1}

\subsection*{Question 2}

\end{document}
